#!/usr/bin/env python3
"""
Redditæ•°æ®æ”¶é›†è„šæœ¬
ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­é…ç½®çš„Reddit APIå‡­æ®æ”¶é›†æ•°æ®
"""

import os
import json
import praw
from datetime import datetime, timedelta
import time
from typing import List, Dict
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class RedditDataCollector:
    def __init__(self):
        """åˆå§‹åŒ–Reddit APIå®¢æˆ·ç«¯"""
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        client_id = os.getenv('REDDIT_CLIENT_ID')
        client_secret = os.getenv('REDDIT_CLIENT_SECRET')
        user_agent = os.getenv('REDDIT_USER_AGENT', 'TradingAgents:v1.0.0')
        
        if not client_id or not client_secret:
            raise ValueError("è¯·ç¡®ä¿å·²è®¾ç½® REDDIT_CLIENT_ID å’Œ REDDIT_CLIENT_SECRET ç¯å¢ƒå˜é‡")
        
        print(f"åˆå§‹åŒ–Redditå®¢æˆ·ç«¯...")
        print(f"Client ID: {client_id}")
        print(f"User Agent: {user_agent}")
        
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
        
        # ç¡®ä¿åªè¯»æ¨¡å¼
        self.reddit.read_only = True
        
        # éªŒè¯è¿æ¥
        try:
            print("âœ… Reddit APIè¿æ¥æˆåŠŸ!")
        except Exception as e:
            print(f"âŒ Reddit APIè¿æ¥å¤±è´¥: {e}")
            raise

    def collect_subreddit_posts(self, subreddit_name: str, limit: int = 50, 
                              time_filter: str = 'week') -> List[Dict]:
        """æ”¶é›†æŒ‡å®šsubredditçš„å¸–å­"""
        try:
            subreddit = self.reddit.subreddit(subreddit_name)
            posts = []
            
            print(f"ğŸ“° æ­£åœ¨æ”¶é›† r/{subreddit_name} çš„å¸–å­...")
            
            for submission in subreddit.top(time_filter=time_filter, limit=limit):
                post_data = {
                    'created_utc': int(submission.created_utc),
                    'title': submission.title,
                    'selftext': submission.selftext,
                    'url': submission.url,
                    'ups': submission.ups,
                    'upvote_ratio': submission.upvote_ratio,
                    'num_comments': submission.num_comments,
                    'id': submission.id,
                    'subreddit': str(submission.subreddit)
                }
                posts.append(post_data)
                
                # éµå®ˆAPIé™åˆ¶ï¼šæ¯ç§’æœ€å¤š1ä¸ªè¯·æ±‚
                time.sleep(1)
            
            print(f"âœ… æ”¶é›†å®Œæˆï¼š{len(posts)} ä¸ªå¸–å­")
            return posts
            
        except Exception as e:
            print(f"âŒ æ”¶é›† r/{subreddit_name} æ—¶å‡ºé”™: {e}")
            return []

    def save_to_jsonl(self, posts: List[Dict], filepath: str):
        """ä¿å­˜å¸–å­æ•°æ®ä¸º.jsonlæ ¼å¼"""
        os.makedirs(os.path.dirname(filepath), exist_ok=True)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for post in posts:
                f.write(json.dumps(post, ensure_ascii=False) + '\n')
        
        print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {filepath}")

def main():
    """ä¸»å‡½æ•°ï¼šæ”¶é›†Redditæ•°æ®"""
    try:
        collector = RedditDataCollector()
        
        # å…¨çƒæ–°é—»subredditsï¼ˆç”¨äºæ–°é—»åˆ†æå¸ˆï¼‰
        print("\nğŸŒ å¼€å§‹æ”¶é›†å…¨çƒæ–°é—»æ•°æ®...")
        global_news_subreddits = ['worldnews', 'economics', 'business']
        
        for subreddit in global_news_subreddits:
            posts = collector.collect_subreddit_posts(subreddit, limit=30, time_filter='week')
            if posts:
                filepath = f'data/reddit_data/global_news/{subreddit}.jsonl'
                collector.save_to_jsonl(posts, filepath)
        
        # è‚¡ç¥¨ç›¸å…³subredditsï¼ˆç”¨äºç¤¾äº¤åª’ä½“åˆ†æå¸ˆï¼‰  
        print("\nğŸ“ˆ å¼€å§‹æ”¶é›†è‚¡ç¥¨æ–°é—»æ•°æ®...")
        stock_subreddits = ['stocks', 'investing', 'SecurityAnalysis']
        
        for subreddit in stock_subreddits:
            posts = collector.collect_subreddit_posts(subreddit, limit=30, time_filter='week')
            if posts:
                filepath = f'data/reddit_data/company_news/{subreddit}.jsonl'
                collector.save_to_jsonl(posts, filepath)
        
        print("\nğŸ‰ Redditæ•°æ®æ”¶é›†å®Œæˆï¼")
        print("ç°åœ¨å¯ä»¥åœ¨TradingAgentsä¸­ä½¿ç”¨Redditå·¥å…·äº†ã€‚")
        
    except Exception as e:
        print(f"\nâŒ æ”¶é›†è¿‡ç¨‹å‡ºé”™: {e}")
        print("è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚")

if __name__ == "__main__":
    main()
